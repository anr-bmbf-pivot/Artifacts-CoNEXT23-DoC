# Data Collection Scripts for Empirical View on IoT DNS Traffic

This directory contains scripts to generate CSV files compatible with the scripts in
[`03-dns-empirical/plot`](../plot) which utilize the [Pandas] library. It contains 4 scripts:

- [`scan_iot_data.py`](#scan_iot_datapy) to transform collected IoT data in
  [PCAP] files into [Pandas]-compatible CSV files.
- [`run_parallel_ixp_dns.sh`](#run_parallel_ixp_dnssh) to transform collected remote IXP
  data in [PCAP] files into anonymized CSV files.
- [`reformat_dns_week_2022_2.py`](#reformat_dns_week_2022_2py) takes the IXP data reformatted by
  `run_parallel_ixp_dns.sh` and parses them into a [Pandas]-compatible CSV format
- [`count_names.sh`](#count_namessh) can be used to gauge the number of unique names in the CSV files
  generated by `scan_iot_data.py`.
- [`count_tshark_names.sh`](#count_tshark_namessh) was used to confirm that using [TShark] generates
  the same set of unique names as `scan_iot_data.py`.

## Requirements
The scripts all were tested on Ubuntu 22.04. While the scripts should be possible to run in other
operating systems (especially the Python scripts), we do not guarantee successful execution.
To run the commands described below, first run, e.g., `apt` on Ubuntu 22.04 to install dependencies:

```
sudo apt update
sudo apt install python3-pip python3-virtualenv
```

All required python libraries are listed in [`requirements.txt`](./requirements.txt). They can be
installed using [pip] with the commands below.
We recommend installing them to a [Virtualenv] as shown, but it is not strictly necessary.

```sh
virtualenv env
. env/bin/activate
pip install -r requirements.txt
```

To generate the IXP data set, you also need [TShark], the [sFlow Toolkit] as well as access to the
collected sFlow samples of an IXP.

To execute [`count_tshark_names.sh`](#count_tshark_namessh), [TShark] and [GNU Parallel] need to be
installed. On Ubuntu 22.04 this can be done, e.g., with `apt`:

```sh
sudo apt install tshark parallel
```

## Testing

The python scripts are tested for python versions 3.7 to 3.11 using [tox]. To test and lint the
code, run the following in this directory ([`03-dns-empirical/collect`](./)). If the python version
under test is installed, the tests for it will be executed.

```sh
tox
```

The [pytest] test cases can be found in the [`tests/`](./tests) directory.

## [`scan_iot_data.py`](./scan_iot_data.py)

`scan_iot_data.py` transforms the [PCAP] files provided by the [YourThings], [IoTFinder], and
[MonIoTr] studies into CSV files that can be parsed by the [Pandas] scripts in
[`03-dns-empirical/plot`](../plot). It expects either a tar file or a directory as input. Depending
on the source data set, the execution may take a long time.

```sh
./scan_iot_data.py ../results/iotfinder-iot-data/
# resulting CSV will be named ../results/iotfinder-iot-data.csv
./scan_iot_data.py ../results/moniotr-iot-data.tgz
# resulting CSV will be named ../results/moniotr-iot-data.tgz.csv
```

## [`run_parallel_ixp_dns.sh`](./run_parallel_ixp_dns.sh)

`run_parallel_ixp_dns.sh` transforms sampling data from an IXP to an intermediate, pseudonymized CSV
format. The IXP data is expected to be stored as [PCAP] files in `/mnt/data01/tcpdumpFiles`. The
location can be changed by modifying the `LOGDIR` environment variable.

The script only takes files that were modified between 2022-01-10 and 2022-01-17 (the second
calendar week of 2022) into account. For another time span, modify the `TS_START` and `TS_END`
environment variables. `TS_START` is the start date for the PCAPs and `TS_END` the last date for the
PCAPs to take into account.

```sh
LOGDIR=./myTCPdumps/ TS_START=2022-01-10 TS_END=2022-01-17 ./run_parallel_ixp_dns.sh
```

The resulting `.csv.gz` file will be stored to `dns_packets_ixp_2022_week.csv.gz`.

## [`reformat_dns_week_2022_2.py`](./reformat_dns_week_2022_2.py)

`reformat_dns_week_2022_2.py` will provide the IXP data generated with `run_parallel_ixp_dns.sh` in
a format that can be parsed by the [Pandas] scripts in [`03-dns-empirical/plot`](../plot). It
expects the `.csv.gz` file generated with `run_parallel_ixp_dns.sh` as input (which is also provided
by us in [`results/ixp-data-set`](../results/ixp-data-set/)):

```sh
./reformat_dns_week_2022_2.py ../results/ixp-data-set/dns_packets_ixp_2022_week.csv.gz
```

The resulting `.csv.gz` file will be stored in `../results/dns_packets_ixp_2022_week.csv.gz`.

## [`count_names.sh`](./count_names.sh)

`count_names.sh` can be used to gauge the number unique of names in the CSV files generated by
[`scan_iot_data.py`](#scan_iot_datapy). It expects the CSV file as input and prints the number of
unique names with various filters to stdout.

## [`count_tshark_names.sh`](./count_tshark_names.sh)

`count_tshark_names.sh` was used to confirm that our [`scan_iot_data.py`](#scan_iot_datapy) script
generates the same set of unique names when using [TShark] instead of [Scapy]. This method was used
in a yet to be published follow-up study of ours.

It requires [TShark] and [GNU Parallel] to be installed and the following directory structure in the
directory the script is executed:

- `.`
    - `iotfinder`: The _unpacked_ contents of the tarballs of the [IoTFinder] study
    - `moniotr`: The _unpacked_ contents of the tarball of the [MonIoTr] study
    - `yourthings`: The _unpacked_ contents of the tarballs of the [YourThings] study

Those directories are called the _input directories_ in the following.

With the selected tools tools and since the contents of the tarballs are required to be unpacked, it
is significantly faster than the [Scapy]-based `scan_iot_data.py` script, but the input data
requires more hard disk space and the script itself yields a smaller amount of data points (see the
intermediary results below).

To execute run

```sh
./count_tshark_names.sh
```

As intermediary results it generates the following files in the directory the script is executed

- a directory `names_addr_w_mdns/` that contains one CSV for each PCAP found in the input
  directories. The CSVs have the following columns, which are separated by semicolons (`;`), based
  on [TShark] field designators:
  - `dataset`: The name of the input directory the original PCAP was found in.
  - `_ws.col.Source`: The IP source address of a DNS message.
  - `_ws.col.Destination`: The IP destination address of a DNS message.
  - `dns.qry.name`: The queried name(s) in the DNS message. If more than one name was queried by the
    DNS message, they are separated by a `|` delimiter.
- a file `names_addr_w_mdns.csv` which is the concatenation of all CSVs in `names_addr_w_mdns/`.
- a file `names_w_mdns_filtered.csv` which only contains the `dns.qry.name` column of
  `names_addr_w_mdns.csv` after filtering the source and destination addresses for
  `EXCLUDED_DEVICES`

The outputs the number of unique queried names for all devices (before filtering for
`EXCLUDED_DEVICES`) and without (w/o) the excluded devices (after filtering for `EXCLUDED_DEVICES`).

[PCAP]: https://www.tcpdump.org/
[pip]: https://pip.pypa.io
[Virtualenv]: https://virtualenv.pypa.io
[TShark]: https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html
[sFlow Toolkit]: https://inmon.com/technology/sflowTools.php 
[tox]: https://tox.wiki
[pytest]: https://docs.pytest.org
[Pandas]: https://pandas.pydata.org/
[YourThings]: https://yourthings.info/data/#yourthings-data
[IoTFinder]: https://yourthings.info/data/#iotfinder-data
[MonIoTr]: https://moniotrlab.ccis.neu.edu/imc19/
[Scapy]: https://scapy.net
[GNU Parallel]: https://www.gnu.org/software/parallel/
